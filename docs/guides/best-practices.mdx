---
title: "Best Practices & Production Guidelines"
description: "Comprehensive best practices for deploying, securing, and maintaining IBM i MCP Server in production environments with performance optimization and operational excellence guidelines."
---

# Best Practices & Production Guidelines

This guide provides comprehensive best practices for deploying and maintaining the IBM i MCP Server in production environments. Follow these guidelines to ensure security, performance, reliability, and operational excellence.

<Warning>
Production deployments require careful planning and adherence to these practices. Skipping security or performance guidelines can lead to system vulnerabilities or operational issues.
</Warning>

## Architecture & Design Principles

### 1. Follow the "Logic Throws, Handler Catches" Pattern

This architectural principle is fundamental to the server's design and must be maintained:

<Tabs>
<Tab title="✅ Correct Pattern">
**Logic Layer (logic.ts)**

```typescript
// ✅ Pure business logic that throws structured errors
export async function customerLookupLogic(
  params: CustomerLookupInput,
  context: RequestContext
): Promise<CustomerLookupResponse> {
  // Validate business rules
  if (!params.customerId && !params.email) {
    throw new McpError(
      BaseErrorCode.VALIDATION_ERROR,
      "Either customer ID or email must be provided",
      { toolName: "customer_lookup" }
    );
  }
  
  // Execute business logic
  const result = await IBMiConnectionPool.executeQuery(sql, params, context);
  
  if (!result.success) {
    throw new McpError(
      BaseErrorCode.DATABASE_ERROR, 
      "Customer lookup failed",
      { sqlReturnCode: result.sql_rc }
    );
  }
  
  return processResult(result.data);
}
```

**Handler Layer (registration.ts)**

```typescript
// ✅ Handler catches all errors and formats responses
async (params: CustomerLookupInput, callContext: Record<string, unknown>) => {
  const handlerContext = requestContextService.createRequestContext({
    parentContext: callContext,
    operation: "HandleCustomerLookup",
    input: params,
  });

  try {
    // Invoke logic layer
    const result = await customerLookupLogic(params, handlerContext);
    
    // Format success response
    return {
      structuredContent: result,
      content: [{ type: "text", text: `Found ${result.customers.length} customers` }],
    };
  } catch (error) {
    // Process error and format error response
    const mcpError = ErrorHandler.handleError(error, {
      operation: "tool:customer_lookup",
      context: handlerContext,
      input: params,
    }) as McpError;

    return {
      isError: true,
      content: [{ type: "text", text: `Error: ${mcpError.message}` }],
      structuredContent: {
        code: mcpError.code,
        message: mcpError.message,
        details: mcpError.details,
      },
    };
  }
}
```
</Tab>

<Tab title="❌ Anti-Pattern">
**Avoid These Patterns**

```typescript
// ❌ Logic layer handling its own errors (anti-pattern)
export async function badLogicExample(params: any) {
  try {
    const result = await database.query(sql);
    return {
      success: true,
      data: result,
      content: [{ type: "text", text: "Success!" }]  // Handler responsibility
    };
  } catch (error) {
    return {
      success: false,
      error: error.message,
      content: [{ type: "text", text: "Error!" }]   // Handler responsibility
    };
  }
}

// ❌ Handler not catching errors (anti-pattern)
async (params: any) => {
  const result = await badLogicExample(params);  // Unhandled exceptions escape
  return result;
}
```
</Tab>
</Tabs>

### 2. Proper Error Handling

<CardGroup cols={2}>
  <Card title="Structured Error Types" icon="exclamation-triangle">
    **Use McpError for Consistency**
    
    Always use the structured `McpError` class with appropriate error codes and contextual details.
    
    ```typescript
    throw new McpError(
      BaseErrorCode.VALIDATION_ERROR,
      "User-friendly error message",
      { 
        parameter: "email",
        violation: "Invalid format",
        context: additionalData 
      }
    );
    ```
  </Card>
  
  <Card title="Context Preservation" icon="link">
    **Maintain Request Context**
    
    Always pass RequestContext through the call stack for proper tracing and logging.
    
    ```typescript
    const context = requestContextService.createRequestContext({
      operation: "CustomerLookup",
      toolName: "customer_lookup"
    });
    
    logger.debug("Starting customer lookup", {
      ...context,
      inputParams: params
    });
    ```
  </Card>
</CardGroup>

### 3. Security-First Development

#### Input Validation and Sanitization

<CodeGroup>
```typescript Zod Schema Validation
// ✅ Comprehensive input validation
export const CustomerInputSchema = z.object({
  customerId: z
    .number()
    .int()
    .min(1, "Customer ID must be positive")
    .max(999999999, "Customer ID too large")
    .optional(),
    
  email: z
    .string()
    .email("Invalid email format")
    .max(255, "Email too long")
    .transform(email => email.toLowerCase().trim())
    .optional(),
    
  includeInactive: z
    .boolean()
    .default(false),
    
  limit: z
    .number()
    .int()
    .min(1, "Limit must be at least 1")
    .max(1000, "Limit cannot exceed 1000")
    .default(50)
}).refine(
  data => data.customerId !== undefined || data.email !== undefined,
  {
    message: "Either customerId or email must be provided",
    path: ["customerId", "email"]
  }
);
```

```typescript SQL Parameterization
// ✅ Always use parameterized queries
const sql = `
  SELECT customer_id, name, email, status
  FROM customers 
  WHERE (@customerId IS NULL OR customer_id = @customerId)
    AND (@email IS NULL OR LOWER(email) = LOWER(@email))
    AND (@includeInactive = true OR status = 'ACTIVE')
  ORDER BY customer_id
  FETCH FIRST @limit ROWS ONLY
`;

// Parameters are safely bound by the connection pool
const result = await IBMiConnectionPool.executeQuery(sql, {
  customerId: params.customerId || null,
  email: params.email || null,
  includeInactive: params.includeInactive,
  limit: params.limit
}, context);
```

```typescript Security Validation
// ✅ Additional security checks
function validateSqlSecurity(sql: string): void {
  const normalizedSql = sql.trim().toUpperCase();
  
  // Block dangerous operations
  const restrictedKeywords = [
    'DROP', 'DELETE', 'TRUNCATE', 'ALTER', 
    'CREATE', 'INSERT', 'UPDATE', 'GRANT', 'REVOKE'
  ];
  
  for (const keyword of restrictedKeywords) {
    if (normalizedSql.startsWith(keyword)) {
      throw new McpError(
        BaseErrorCode.VALIDATION_ERROR,
        `SQL operation '${keyword}' is not allowed`,
        { keyword, sqlPrefix: sql.substring(0, 50) }
      );
    }
  }
  
  // Check for injection patterns
  const dangerousPatterns = [
    /;\s*(DROP|DELETE|TRUNCATE|ALTER)/i,
    /UNION\s+SELECT.*INTO/i,
    /EXEC\s*\(/i
  ];
  
  for (const pattern of dangerousPatterns) {
    if (pattern.test(sql)) {
      throw new McpError(
        BaseErrorCode.VALIDATION_ERROR,
        "SQL contains potentially dangerous patterns",
        { pattern: pattern.source }
      );
    }
  }
}
```
</CodeGroup>

#### Authentication and Authorization

<Tabs>
<Tab title="JWT Authentication">
**Production JWT Configuration**

```typescript
// Environment configuration
const jwtConfig = {
  secretKey: process.env.MCP_AUTH_SECRET_KEY, // Min 32 characters
  algorithm: 'HS256',
  expiresIn: '24h',
  issuer: 'ibmi-mcp-server',
  audience: 'mcp-clients'
};

// Token validation middleware
export const withJWTAuth = (requiredScopes: string[]) => {
  return async (params: any, context: any) => {
    const token = extractTokenFromContext(context);
    
    try {
      const decoded = jwt.verify(token, jwtConfig.secretKey) as JwtPayload;
      
      // Validate scopes
      const tokenScopes = decoded.scope?.split(' ') || [];
      const hasRequiredScopes = requiredScopes.every(scope => 
        tokenScopes.includes(scope)
      );
      
      if (!hasRequiredScopes) {
        throw new McpError(
          BaseErrorCode.AUTHORIZATION_ERROR,
          "Insufficient permissions",
          { requiredScopes, tokenScopes }
        );
      }
      
      // Add user context
      context.userId = decoded.sub;
      context.scopes = tokenScopes;
      
    } catch (error) {
      throw new McpError(
        BaseErrorCode.AUTHENTICATION_ERROR,
        "Invalid or expired token",
        { tokenError: error.message }
      );
    }
  };
};
```
</Tab>

<Tab title="Scope Protection">
**Tool-Level Authorization**

```typescript
// Protect tools with specific scopes
server.registerTool(
  "customer_lookup",
  toolSchema,
  withRequiredScopes(["ibmi:read"], async (params, context) => {
    // Only users with 'ibmi:read' scope can execute this tool
    return await customerLookupLogic(params, context);
  })
);

server.registerTool(
  "execute_sql", 
  toolSchema,
  withRequiredScopes(["ibmi:execute"], async (params, context) => {
    // Dynamic SQL requires higher privileges
    return await executeSqlLogic(params, context);
  })
);

server.registerTool(
  "user_management",
  toolSchema, 
  withRequiredScopes(["ibmi:admin"], async (params, context) => {
    // Administrative operations require admin scope
    return await userManagementLogic(params, context);
  })
);
```
</Tab>
</Tabs>

## Performance Optimization

### 1. Database Connection Management

<CardGroup cols={2}>
  <Card title="Connection Pooling" icon="database">
    **Optimize Pool Configuration**
    
    ```typescript
    const poolConfig = {
      max: 20,          // Maximum connections
      min: 5,           // Minimum connections
      acquire: 30000,   // Max time to get connection (30s)
      idle: 10000,      // Max idle time (10s)
      evict: 1000,      // Eviction check interval (1s)
      handleDisconnects: true,
      reconnect: true
    };
    ```
  </Card>
  
  <Card title="Query Optimization" icon="zap">
    **SQL Best Practices**
    
    - Use appropriate WHERE clauses with indexed columns
    - Implement FETCH FIRST clauses for large result sets
    - Avoid SELECT * in production queries
    - Use EXISTS instead of IN for large subqueries
  </Card>
</CardGroup>

### 2. Caching Strategies

<CodeGroup>
```typescript Result Caching
export class ResultCache {
  private cache = new Map<string, CachedResult>();
  private readonly TTL = 5 * 60 * 1000; // 5 minutes
  
  async get<T>(key: string, factory: () => Promise<T>): Promise<T> {
    const cached = this.cache.get(key);
    
    if (cached && (Date.now() - cached.timestamp) < this.TTL) {
      return cached.data as T;
    }
    
    const result = await factory();
    this.cache.set(key, {
      data: result,
      timestamp: Date.now()
    });
    
    return result;
  }
  
  invalidate(pattern?: string): void {
    if (pattern) {
      const regex = new RegExp(pattern);
      for (const [key] of this.cache) {
        if (regex.test(key)) {
          this.cache.delete(key);
        }
      }
    } else {
      this.cache.clear();
    }
  }
}

// Usage in tools
const cache = new ResultCache();

export async function systemStatusLogic(
  params: SystemStatusInput,
  context: RequestContext
): Promise<SystemStatusResponse> {
  const cacheKey = `system_status:${JSON.stringify(params)}`;
  
  return await cache.get(cacheKey, async () => {
    const result = await IBMiConnectionPool.executeQuery(
      "SELECT * FROM QSYS2.SYSTEM_STATUS()",
      [],
      context
    );
    
    return processSystemStatus(result.data);
  });
}
```

```yaml YAML Tool Caching
# Enable caching in YAML tools
tools:
  system_status:
    source: production-db
    description: "Get current system status"
    cache:
      enabled: true
      ttl: 300  # 5 minutes
      key_template: "system_status:{timestamp_hour}"  # Cache by hour
    statement: |
      SELECT * FROM QSYS2.SYSTEM_STATUS()
```
</CodeGroup>

### 3. Resource Management

<Tabs>
<Tab title="Memory Management">
**Prevent Memory Leaks**

```typescript
export class ResourceManager {
  private resources: Set<Disposable> = new Set();
  
  register<T extends Disposable>(resource: T): T {
    this.resources.add(resource);
    return resource;
  }
  
  async dispose(): Promise<void> {
    const disposePromises = Array.from(this.resources).map(async resource => {
      try {
        await resource.dispose();
      } catch (error) {
        logger.error("Resource disposal error", { error: error.message });
      }
    });
    
    await Promise.all(disposePromises);
    this.resources.clear();
  }
}

// Use in tools
export async function longRunningToolLogic(
  params: LongRunningInput,
  context: RequestContext
): Promise<LongRunningResponse> {
  const resourceManager = new ResourceManager();
  
  try {
    const connection = resourceManager.register(
      await IBMiConnectionPool.acquire()
    );
    
    const tempFile = resourceManager.register(
      new TempFile(`/tmp/processing_${context.requestId}.tmp`)
    );
    
    // Process data
    const result = await processLargeDataset(connection, tempFile, params);
    
    return result;
  } finally {
    await resourceManager.dispose();
  }
}
```
</Tab>

<Tab title="Concurrent Operations">
**Handle Concurrency Properly**

```typescript
// Rate limiting for resource-intensive operations
export class RateLimiter {
  private requests = new Map<string, number[]>();
  
  async checkLimit(
    key: string, 
    maxRequests: number, 
    windowMs: number
  ): Promise<boolean> {
    const now = Date.now();
    const windowStart = now - windowMs;
    
    const requests = this.requests.get(key) || [];
    const validRequests = requests.filter(time => time > windowStart);
    
    if (validRequests.length >= maxRequests) {
      return false;
    }
    
    validRequests.push(now);
    this.requests.set(key, validRequests);
    return true;
  }
}

// Apply rate limiting to tools
const rateLimiter = new RateLimiter();

export const withRateLimit = (maxRequests: number, windowMs: number) => {
  return async (params: any, context: RequestContext) => {
    const clientId = context.clientId || 'anonymous';
    
    const allowed = await rateLimiter.checkLimit(
      clientId, 
      maxRequests, 
      windowMs
    );
    
    if (!allowed) {
      throw new McpError(
        BaseErrorCode.RATE_LIMIT_EXCEEDED,
        "Rate limit exceeded",
        { maxRequests, windowMs }
      );
    }
  };
};
```
</Tab>
</Tabs>

## Production Deployment

### 1. Environment Configuration

<CodeGroup>
```bash Production Environment
# Production environment variables
export NODE_ENV=production
export MCP_LOG_LEVEL=info
export MCP_AUTH_MODE=jwt
export MCP_AUTH_SECRET_KEY="your-very-secure-secret-key-at-least-32-characters"

# Database configuration
export DB2i_HOST="production.ibmi.company.com"
export DB2i_USER="mcp_service_account"
export DB2i_PASS="secure-service-account-password"
export DB2i_PORT=8076

# HTTP server configuration
export MCP_HTTP_HOST=0.0.0.0
export MCP_HTTP_PORT=3010
export MCP_ALLOWED_ORIGINS="https://app.company.com,https://admin.company.com"

# Observability
export OTEL_ENABLED=true
export OTEL_SERVICE_NAME="ibmi-mcp-server"
export OTEL_EXPORTER_OTLP_ENDPOINT="https://otlp.company.com"

# Health checks
export HEALTH_CHECK_ENABLED=true
export HEALTH_CHECK_INTERVAL=30000
```

```dockerfile Production Dockerfile
FROM node:18-slim AS builder

WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

FROM node:18-slim AS runtime

# Create non-root user
RUN groupadd -r mcp && useradd -r -g mcp -d /app -s /sbin/nologin mcp

WORKDIR /app

# Copy application files
COPY --from=builder /app/node_modules ./node_modules
COPY dist/ ./dist/
COPY package*.json ./

# Set ownership
RUN chown -R mcp:mcp /app

# Switch to non-root user
USER mcp

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:3010/health || exit 1

EXPOSE 3010

CMD ["node", "dist/index.js"]
```

```yaml Kubernetes Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ibmi-mcp-server
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ibmi-mcp-server
  template:
    metadata:
      labels:
        app: ibmi-mcp-server
    spec:
      containers:
      - name: mcp-server
        image: company/ibmi-mcp-server:v1.9.1
        ports:
        - containerPort: 3010
        env:
        - name: NODE_ENV
          value: "production"
        - name: MCP_AUTH_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: mcp-secrets
              key: jwt-secret
        - name: DB2i_HOST
          valueFrom:
            configMapKeyRef:
              name: mcp-config
              key: database-host
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 3010
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 3010
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: ibmi-mcp-server
  namespace: production
spec:
  selector:
    app: ibmi-mcp-server
  ports:
  - protocol: TCP
    port: 80
    targetPort: 3010
```
</CodeGroup>

### 2. Monitoring and Observability

<CodeGroup>
```typescript Health Checks
// Comprehensive health check implementation
export class HealthChecker {
  async checkHealth(): Promise<HealthStatus> {
    const checks = await Promise.allSettled([
      this.checkDatabase(),
      this.checkMemory(),
      this.checkDiskSpace(),
      this.checkExternalServices()
    ]);
    
    const failures = checks
      .filter(result => result.status === 'rejected')
      .map(result => (result as PromiseRejectedResult).reason);
    
    return {
      status: failures.length === 0 ? 'healthy' : 
              failures.length < checks.length ? 'degraded' : 'unhealthy',
      timestamp: new Date().toISOString(),
      checks: {
        database: checks[0].status === 'fulfilled' ? 'pass' : 'fail',
        memory: checks[1].status === 'fulfilled' ? 'pass' : 'fail', 
        diskSpace: checks[2].status === 'fulfilled' ? 'pass' : 'fail',
        externalServices: checks[3].status === 'fulfilled' ? 'pass' : 'fail'
      },
      errors: failures.map(f => f.message)
    };
  }
  
  private async checkDatabase(): Promise<void> {
    const result = await IBMiConnectionPool.executeQuery(
      "SELECT 1 FROM SYSIBM.SYSDUMMY1",
      [],
      { requestId: 'health-check', operation: 'DatabaseHealth' }
    );
    
    if (!result.success) {
      throw new Error(`Database health check failed: ${result.error}`);
    }
  }
}
```

```typescript Metrics Collection
// Performance metrics
export class MetricsCollector {
  private registry = new prometheus.Registry();
  
  constructor() {
    // Tool execution metrics
    this.toolExecutionCounter = new prometheus.Counter({
      name: 'mcp_tool_executions_total',
      help: 'Total number of tool executions',
      labelNames: ['tool_name', 'status'],
      registers: [this.registry]
    });
    
    this.toolExecutionDuration = new prometheus.Histogram({
      name: 'mcp_tool_execution_duration_seconds',
      help: 'Tool execution duration',
      labelNames: ['tool_name'],
      buckets: [0.1, 0.5, 1, 2, 5, 10],
      registers: [this.registry]
    });
    
    // Database connection metrics
    this.dbConnectionPool = new prometheus.Gauge({
      name: 'mcp_db_connections_active',
      help: 'Active database connections',
      registers: [this.registry]
    });
  }
  
  recordToolExecution(toolName: string, duration: number, success: boolean): void {
    this.toolExecutionCounter
      .labels(toolName, success ? 'success' : 'error')
      .inc();
      
    this.toolExecutionDuration
      .labels(toolName)
      .observe(duration);
  }
  
  updateConnectionPoolMetrics(active: number): void {
    this.dbConnectionPool.set(active);
  }
}
```

```typescript Structured Logging
// Production logging configuration
const logger = winston.createLogger({
  level: process.env.MCP_LOG_LEVEL || 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json()
  ),
  transports: [
    new winston.transports.File({ 
      filename: '/var/log/mcp/error.log', 
      level: 'error' 
    }),
    new winston.transports.File({ 
      filename: '/var/log/mcp/combined.log' 
    }),
    ...(process.env.NODE_ENV === 'production' ? [] : [
      new winston.transports.Console({
        format: winston.format.simple()
      })
    ])
  ]
});

// Structured logging in tools
export async function toolWithLogging(
  params: ToolInput,
  context: RequestContext
): Promise<ToolResponse> {
  logger.info("Tool execution started", {
    requestId: context.requestId,
    toolName: "example_tool",
    parameters: params,
    timestamp: new Date().toISOString()
  });
  
  const startTime = Date.now();
  
  try {
    const result = await executeBusinessLogic(params, context);
    
    logger.info("Tool execution completed", {
      requestId: context.requestId,
      toolName: "example_tool",
      duration: Date.now() - startTime,
      resultSize: JSON.stringify(result).length,
      success: true
    });
    
    return result;
  } catch (error) {
    logger.error("Tool execution failed", {
      requestId: context.requestId,
      toolName: "example_tool", 
      duration: Date.now() - startTime,
      error: error.message,
      stack: error.stack,
      success: false
    });
    
    throw error;
  }
}
```
</CodeGroup>

## Security Best Practices

### 1. Network Security

<CodeGroup>
```nginx Reverse Proxy Configuration
# /etc/nginx/sites-available/mcp-server
server {
    listen 443 ssl http2;
    server_name mcp.company.com;
    
    # SSL Configuration
    ssl_certificate /etc/ssl/certs/mcp.company.com.pem;
    ssl_certificate_key /etc/ssl/private/mcp.company.com.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;
    ssl_prefer_server_ciphers off;
    
    # Security Headers
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=63072000; includeSubDomains; preload";
    
    # Rate Limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=100r/m;
    limit_req zone=api burst=20 nodelay;
    
    location / {
        proxy_pass http://localhost:3010;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Preserve Authorization header
        proxy_pass_header Authorization;
        
        # Timeout configuration
        proxy_connect_timeout 5s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }
    
    # Health check endpoint (no auth required)
    location /health {
        proxy_pass http://localhost:3010/health;
        access_log off;
    }
}
```

```yaml Security Scanning
# security-scan.yml - CI/CD security checks
name: Security Scan
on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  security-scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Run npm audit
      run: npm audit --audit-level=moderate
      
    - name: Run Snyk security scan
      uses: snyk/actions/node@master
      env:
        SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
        
    - name: Run SAST scan
      uses: securecodewarrior/github-action-add-sarif@v1
      with:
        sarif-file: security-scan-results.sarif
        
    - name: Check for secrets
      uses: trufflesecurity/trufflehog@main
      with:
        path: ./
        base: ${{ github.event.repository.default_branch }}
        head: HEAD
```
</CodeGroup>

### 2. Data Protection

<CardGroup cols={2}>
  <Card title="Encryption at Rest" icon="lock">
    **Database Encryption**
    
    - Use encrypted IBM i database features
    - Encrypt sensitive configuration files
    - Secure log file storage
    - Regular key rotation procedures
  </Card>
  
  <Card title="Encryption in Transit" icon="shield-alt">
    **Network Security**
    
    - TLS 1.2+ for all connections
    - Certificate pinning for external services
    - VPN or private networks for database connections
    - Secure WebSocket connections
  </Card>
</CardGroup>

## Operational Excellence

### 1. Deployment Strategies

<Tabs>
<Tab title="Blue-Green Deployment">
**Zero-Downtime Deployments**

```bash
#!/bin/bash
# blue-green-deploy.sh

CURRENT_COLOR=$(kubectl get service mcp-server -o jsonpath='{.spec.selector.version}')
NEW_COLOR=$([ "$CURRENT_COLOR" = "blue" ] && echo "green" || echo "blue")

echo "Current deployment: $CURRENT_COLOR"
echo "Deploying to: $NEW_COLOR"

# Deploy new version
kubectl apply -f deployment-$NEW_COLOR.yaml

# Wait for deployment to be ready
kubectl rollout status deployment/mcp-server-$NEW_COLOR

# Run health checks
if ./health-check.sh mcp-server-$NEW_COLOR; then
  echo "Health checks passed, switching traffic"
  
  # Switch traffic
  kubectl patch service mcp-server -p '{"spec":{"selector":{"version":"'$NEW_COLOR'"}}}'
  
  echo "Traffic switched to $NEW_COLOR"
  
  # Clean up old deployment after grace period
  sleep 300
  kubectl delete deployment mcp-server-$CURRENT_COLOR
else
  echo "Health checks failed, rolling back"
  kubectl delete deployment mcp-server-$NEW_COLOR
  exit 1
fi
```
</Tab>

<Tab title="Canary Deployment">
**Gradual Traffic Migration**

```yaml
# canary-deployment.yaml
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: ibmi-mcp-server
spec:
  replicas: 5
  strategy:
    canary:
      steps:
      - setWeight: 20
      - pause: {duration: 10m}
      - setWeight: 40  
      - pause: {duration: 10m}
      - setWeight: 60
      - pause: {duration: 10m}
      - setWeight: 80
      - pause: {duration: 10m}
      canaryService: mcp-server-canary
      stableService: mcp-server-stable
      trafficRouting:
        nginx:
          stableIngress: mcp-server-ingress
  selector:
    matchLabels:
      app: ibmi-mcp-server
  template:
    metadata:
      labels:
        app: ibmi-mcp-server
    spec:
      containers:
      - name: mcp-server
        image: company/ibmi-mcp-server:v1.9.2
```
</Tab>
</Tabs>

### 2. Backup and Recovery

<CodeGroup>
```bash Automated Backups
#!/bin/bash
# backup-configuration.sh

BACKUP_DIR="/backup/mcp-server/$(date +%Y%m%d_%H%M%S)"
mkdir -p "$BACKUP_DIR"

# Backup configuration files
cp -r /app/config "$BACKUP_DIR/"
cp -r /app/toolsets "$BACKUP_DIR/"

# Backup YAML tools
kubectl get configmap mcp-tools-config -o yaml > "$BACKUP_DIR/tools-config.yaml"

# Backup secrets (encrypted)
kubectl get secret mcp-secrets -o yaml | \
  gpg --encrypt --armor --recipient backup@company.com > "$BACKUP_DIR/secrets.yaml.gpg"

# Backup database schemas
./generate-schema-backup.sh > "$BACKUP_DIR/database-schema.sql"

# Create manifest
cat > "$BACKUP_DIR/manifest.json" << EOF
{
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%S.%3NZ)",
  "version": "$(kubectl get deployment mcp-server -o jsonpath='{.spec.template.spec.containers[0].image}')",
  "config_files": [
    "config/",
    "toolsets/",
    "tools-config.yaml",
    "secrets.yaml.gpg",
    "database-schema.sql"
  ]
}
EOF

# Upload to backup storage
aws s3 sync "$BACKUP_DIR" "s3://company-backups/mcp-server/$(basename "$BACKUP_DIR")" --encryption
```

```bash Disaster Recovery
#!/bin/bash
# disaster-recovery.sh

BACKUP_VERSION="$1"
if [ -z "$BACKUP_VERSION" ]; then
  echo "Usage: $0 <backup-version>"
  exit 1
fi

RESTORE_DIR="/tmp/restore-$BACKUP_VERSION"

# Download backup
aws s3 sync "s3://company-backups/mcp-server/$BACKUP_VERSION" "$RESTORE_DIR"

# Verify backup integrity
if ! ./verify-backup.sh "$RESTORE_DIR"; then
  echo "Backup verification failed"
  exit 1
fi

# Restore configuration
kubectl create configmap mcp-tools-config --from-file="$RESTORE_DIR/tools-config.yaml" --dry-run=client -o yaml | kubectl apply -f -

# Restore secrets (decrypt first)
gpg --decrypt "$RESTORE_DIR/secrets.yaml.gpg" | kubectl apply -f -

# Restore database schema if needed
# ./restore-database-schema.sh "$RESTORE_DIR/database-schema.sql"

# Restart deployment to pick up new configuration
kubectl rollout restart deployment/mcp-server

echo "Recovery completed from backup: $BACKUP_VERSION"
```
</CodeGroup>

### 3. Incident Response

<Tabs>
<Tab title="Automated Alerts">
**Monitoring and Alerting**

```yaml
# prometheus-alerts.yml
groups:
- name: mcp-server-alerts
  rules:
  - alert: MCPServerDown
    expr: up{job="mcp-server"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "MCP Server is down"
      description: "MCP Server has been down for more than 1 minute"
      
  - alert: HighErrorRate
    expr: rate(mcp_tool_executions_total{status="error"}[5m]) > 0.1
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "High error rate detected"
      description: "Error rate is {{ $value }} errors per second"
      
  - alert: DatabaseConnectionIssues
    expr: mcp_db_connections_active == 0
    for: 30s
    labels:
      severity: critical
    annotations:
      summary: "No active database connections"
      description: "Database connection pool has no active connections"
```
</Tab>

<Tab title="Incident Runbook">
**Standard Operating Procedures**

```markdown
# MCP Server Incident Response Runbook

## Severity Levels

### P0 - Critical (< 15 min response)
- Complete service outage
- Data corruption or loss
- Security breach

### P1 - High (< 1 hour response)  
- Degraded performance affecting users
- Partial functionality loss
- Authentication issues

### P2 - Medium (< 4 hour response)
- Non-critical feature failures
- Performance degradation
- Configuration issues

## Response Procedures

### Step 1: Immediate Assessment
```bash
# Check service status
kubectl get pods -l app=ibmi-mcp-server
kubectl logs -l app=ibmi-mcp-server --tail=100

# Check health endpoint
curl -f http://mcp.company.com/health

# Check metrics dashboard
open https://grafana.company.com/d/mcp-server
```

### Step 2: Containment
```bash
# Scale up if resource related
kubectl scale deployment ibmi-mcp-server --replicas=6

# Rollback if deployment related
kubectl rollout undo deployment/ibmi-mcp-server

# Enable circuit breaker for downstream services
kubectl patch configmap mcp-config --patch '{"data":{"circuit_breaker":"enabled"}}'
```

### Step 3: Resolution
- Follow specific troubleshooting guides
- Apply fixes and validate
- Monitor for stability

### Step 4: Post-Incident
- Document timeline
- Update runbooks
- Schedule post-mortem
```
</Tab>
</Tabs>

---

<Info>
These best practices represent battle-tested approaches for running IBM i MCP Server in production. Start with the fundamentals (security, monitoring) and gradually implement advanced patterns as your deployment matures.
</Info>
